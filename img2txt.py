# -*- coding: utf-8 -*-
"""img2txt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zjee1g3bnPWffkXZ86usN_1WKl82v1Xy

## Setup
"""

!pip install --upgrade ai2thor --quiet
!pip install ai2thor-colab prior --upgrade &> /dev/null

import os
os.system('apt-get install xvfb')

import prior

dataset = prior.load_dataset("procthor-10k")
dataset

import ai2thor_colab
ai2thor_colab.start_xserver()

house = dataset["train"][11]

from ai2thor.controller import Controller

controller = Controller(scene=house, visibilityDistance=2, width=750, height=750)

from PIL import Image
Image.fromarray(controller.last_event.frame)

img2text = Image.fromarray(controller.last_event.frame)

!pip install transformers

from transformers import pipeline

import warnings,logging

def img2txt(image):
  warnings.simplefilter('ignore')
  logging.disable(logging.WARNING)
  caption = pipeline('image-to-text')
  text = caption(image)
  return text

img2txt(img2text)

import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'

# unconditional image captioning
inputs = processor(img2text, return_tensors="pt")

out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))